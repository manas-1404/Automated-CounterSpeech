{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-o6cOS8nfmT_",
        "outputId": "28defc27-31d1-4719-e6e1-05465c53b0dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vaderSentiment) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2023.11.17)\n",
            "Installing collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install vaderSentiment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "testSentence = SentimentIntensityAnalyzer()\n",
        "print(testSentence.polarity_scores(\"Water is a vital nutrient\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lgxv3tdGqgUg",
        "outputId": "a8f25609-c97c-406e-a704-b81757d65d18"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'neg': 0.0, 'neu': 0.645, 'pos': 0.355, 'compound': 0.296}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ^Issue: does this really make sense? should we compare with others?\n",
        "# NEXT: come up with procedural rules,"
      ],
      "metadata": {
        "id": "UhrKISO8qvmp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def personalOpinion(sentence):\n",
        "  sentence = \"In my opinion, \" + sentence[0].lower() + sentence[1:]\n",
        "  return sentence"
      ],
      "metadata": {
        "id": "RrQuznmEqJ6G"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nonPersonalOpinion(sentence):\n",
        "  sentence = \"It sounds like \" + sentence[0].lower() + sentence[1:]\n",
        "  return sentence"
      ],
      "metadata": {
        "id": "KhSJx844rCeh"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = \"He eats cheese, but he won't eat ice cream.\"\n",
        "print(testSentence.polarity_scores(data))\n",
        "\n",
        "print(personalOpinion(data))\n",
        "print(testSentence.polarity_scores(personalOpinon(data)))\n",
        "\n",
        "print(nonPersonalOpinion(data))\n",
        "print(testSentence.polarity_scores(nonPersonalOpinon(data)))\n",
        "\n",
        "# print(generalizeFact(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "XjBEr5Cpp4vh",
        "outputId": "8c15d263-8a29-41d0-841a-efed33f8cf40"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
            "In my opinion, he eats cheese, but he won't eat ice cream.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'personalOpinon' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-3553026e1a5b>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpersonalOpinion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestSentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolarity_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpersonalOpinon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnonPersonalOpinion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'personalOpinon' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_rhetorical_question_enhanced(sentence):\n",
        "    \"\"\"\n",
        "    Enhanced function to convert a declarative sentence into a rhetorical question.\n",
        "    This function includes more patterns for conversion.\n",
        "    \"\"\"\n",
        "    # Remove period at the end if it exists\n",
        "    if sentence.endswith('.'):\n",
        "        sentence = sentence[:-1]\n",
        "\n",
        "    # Enhanced conversion rules\n",
        "    if sentence.startswith(\"I am\"):\n",
        "        return \"Aren't I \" + sentence[5:] + \"?\"\n",
        "    elif sentence.startswith(\"He is\") or sentence.startswith(\"She is\"):\n",
        "        return \"Isn't \" + sentence[:3].lower() + \" \" + sentence[6:] + \"?\"\n",
        "    elif sentence.startswith(\"They are\"):\n",
        "        return \"Aren't they \" + sentence[9:] + \"?\"\n",
        "    elif sentence.startswith(\"It is\"):\n",
        "        return \"Isn't it \" + sentence[6:] + \"?\"\n",
        "    elif sentence.startswith(\"You are\"):\n",
        "        return \"Aren't you \" + sentence[8:] + \"?\"\n",
        "    elif sentence.startswith(\"We are\"):\n",
        "        return \"Aren't we \" + sentence[7:] + \"?\"\n",
        "    elif sentence.startswith(\"This is\"):\n",
        "        return \"Isn't this \" + sentence[8:] + \"?\"\n",
        "    elif sentence.startswith(\"There is\"):\n",
        "        return \"Isn't there \" + sentence[9:] + \"?\"\n",
        "    elif sentence.startswith(\"That is\"):\n",
        "        return \"Isn't that \" + sentence[8:] + \"?\"\n",
        "    else:\n",
        "        # A general fallback for sentences not covered by the above patterns\n",
        "        return \"Isn't it true that \" + sentence.lower() + \"?\"\n",
        "\n",
        "# Example sentences with more variety\n",
        "enhanced_example_sentences = [\n",
        "    \"I am excited about the trip.\",\n",
        "    \"He is the best player on the team.\",\n",
        "    \"They are going to win the game.\",\n",
        "    \"It is a beautiful day.\",\n",
        "    \"You are very knowledgeable.\",\n",
        "    \"We are planning a party.\",\n",
        "    \"This is a significant discovery.\",\n",
        "    \"There is a problem with the plan.\",\n",
        "    \"That is a rare bird.\"\n",
        "]\n",
        "\n",
        "# Convert each sentence to a rhetorical question\n",
        "enhanced_rhetorical_questions = [convert_to_rhetorical_question_enhanced(sentence) for sentence in enhanced_example_sentences]\n",
        "enhanced_rhetorical_questions\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtBwNdKXZGpu",
        "outputId": "2cd6592a-63dc-482f-ba5f-6adec246a0bd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Aren't I excited about the trip?\",\n",
              " \"Isn't he  the best player on the team?\",\n",
              " \"Aren't they going to win the game?\",\n",
              " \"Isn't it a beautiful day?\",\n",
              " \"Aren't you very knowledgeable?\",\n",
              " \"Aren't we planning a party?\",\n",
              " \"Isn't this a significant discovery?\",\n",
              " \"Isn't there a problem with the plan?\",\n",
              " \"Isn't that a rare bird?\"]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import random\n",
        "\n",
        "# def make_sentence_opinionated(sentence):\n",
        "#     \"\"\"\n",
        "#     Adds adjectives, adverbs, etc., to make the declarative sentence sound more opinionated.\n",
        "#     This function uses a simple approach by adding common opinionated words.\n",
        "#     \"\"\"\n",
        "\n",
        "#     # Expanded list of adjectives and adverbs to make the sentence more opinionated\n",
        "#     opinionated_adjectives = [\"absolutely\", \"completely\", \"utterly\", \"undeniably\", \"unquestionably\", \"emphatically\", \"categorically\", \"unequivocally\"]\n",
        "#     opinionated_adverbs = [\"incredibly\", \"extremely\", \"highly\", \"remarkably\", \"exceptionally\", \"particularly\", \"profoundly\", \"deeply\"]\n",
        "\n",
        "#     # Split the sentence to insert opinionated words\n",
        "#     words = sentence.split()\n",
        "\n",
        "#     # Randomly select an adjective and an adverb\n",
        "#     selected_adjective = random.choice(opinionated_adjectives)\n",
        "#     selected_adverb = random.choice(opinionated_adverbs)\n",
        "\n",
        "#     # Insert an adjective or adverb after the first word\n",
        "#     if len(words) > 1:\n",
        "#         words.insert(1, selected_adjective)\n",
        "#         words.insert(2, selected_adverb)\n",
        "\n",
        "#     # Reconstruct the sentence\n",
        "#     opinionated_sentence = ' '.join(words)\n",
        "\n",
        "#     return opinionated_sentence\n",
        "\n",
        "# # Example sentence\n",
        "# example_sentence = \"This movie is good.\"\n",
        "# opinionated_sentence = make_sentence_opinionated(example_sentence)\n",
        "\n",
        "# print(opinionated_sentence)\n"
      ],
      "metadata": {
        "id": "hNDzkfVcbJye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "def make_sentence_opinionated(sentence):\n",
        "    \"\"\"\n",
        "    Adds adjectives, adverbs, etc., to make the declarative sentence sound more opinionated.\n",
        "    This function uses a simple approach by adding common opinionated words based on the sentiment of the sentence.\n",
        "    \"\"\"\n",
        "\n",
        "    # Sentiment analysis\n",
        "    analyzer = SentimentIntensityAnalyzer()\n",
        "    sentiment = analyzer.polarity_scores(sentence)\n",
        "\n",
        "    # Lists of adjectives and adverbs based on sentiment\n",
        "    positive_adjectives = [\"absolutely\", \"completely\", \"utterly\", \"undeniably\", \"unquestionably\", \"emphatically\"]\n",
        "    positive_adverbs = [\"incredibly\", \"extremely\", \"highly\", \"remarkably\", \"exceptionally\", \"particularly\"]\n",
        "\n",
        "    negative_adjectives = [\"terribly\", \"horribly\", \"dreadfully\", \"awfully\", \"painfully\", \"woefully\"]\n",
        "    negative_adverbs = [\"severely\", \"deeply\", \"greatly\", \"seriously\", \"intensely\", \"bitterly\"]\n",
        "\n",
        "    # Determine sentiment and select appropriate words\n",
        "    if sentiment['compound'] >= 0.05:  # Positive sentiment\n",
        "        selected_adjective = random.choice(positive_adjectives)\n",
        "        selected_adverb = random.choice(positive_adverbs)\n",
        "    elif sentiment['compound'] <= -0.05:  # Negative sentiment\n",
        "        selected_adjective = random.choice(negative_adjectives)\n",
        "        selected_adverb = random.choice(negative_adverbs)\n",
        "    else:  # Neutral sentiment - use a mix\n",
        "        selected_adjective = random.choice(positive_adjectives + negative_adjectives)\n",
        "        selected_adverb = random.choice(positive_adverbs + negative_adverbs)\n",
        "\n",
        "    # Split the sentence to insert opinionated words\n",
        "    words = sentence.split()\n",
        "\n",
        "    # Insert an adjective or adverb after the first word\n",
        "    if len(words) > 1:\n",
        "        words.insert(1, selected_adjective)\n",
        "        words.insert(2, selected_adverb)\n",
        "\n",
        "    # Reconstruct the sentence\n",
        "    opinionated_sentence = ' '.join(words)\n",
        "\n",
        "    return opinionated_sentence\n",
        "\n",
        "# Example sentence\n",
        "example_sentence = \"This book is bad.\"\n",
        "opinionated_sentence = make_sentence_opinionated(example_sentence)\n",
        "\n",
        "print(opinionated_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYJCKa__FBQV",
        "outputId": "85a4daa4-4aed-469b-cd7c-8ba7639e3f41"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This dreadfully severely book is bad.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcH6erymQPM1",
        "outputId": "2a347300-e346-4d77-a333-a239b03633a2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxgIX8KVRDAv",
        "outputId": "b37a6495-352b-4850-eb8e-c5f9e46ffb05"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# identify main verb and put might/should in front of it\n",
        "from nltk.tokenize import word_tokenize\n",
        "#is_verb = lambda pos: pos[:2] == 'VB'\n",
        "text = 'Horses eat carrots'\n",
        "tokenizedText = word_tokenize(text)\n",
        "print(tokenizedText)\n",
        "print(nltk.pos_tag(tokenizedText))\n",
        "for (word, pos) in nltk.pos_tag(tokenizedText):\n",
        "  if pos == 'VB':\n",
        "    print(pos)\n",
        "\n",
        "#vbs = [word for (word, pos) in nltk.pos_tag(text) if is_verb(pos)]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEihwuI1X62h",
        "outputId": "e1e2e93d-b7e7-46ba-db77-033571853030"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Horses', 'eat', 'carrots']\n",
            "[('Horses', 'NNS'), ('eat', 'VBP'), ('carrots', 'NNS')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "pos_tag(word_tokenize(\"John's big idea isn't all that bad.\"))\n",
        "text = \"Man walks into a bar.\"  # Your text here\n",
        "tokens_positions = list(WhitespaceTokenizer().span_tokenize(text))  # Tokenize to spans to get start/end positions: [(0, 3), (4, 9), ... ]\n",
        "tokens = WhitespaceTokenizer().tokenize(text)  # Tokenize on a string lists: [\"man\", \"walks\", \"into\", ... ]\n",
        "\n",
        "tokens = pos_tag(tokens) # Run Part-of-Speech tager\n",
        "\n",
        "# Iterate on each token\n",
        "words = []\n",
        "for i in range(len(tokens)):\n",
        "    text, tag = tokens[i]  # Get tag\n",
        "    start, end = tokens_positions[i]  # Get token start/end\n",
        "    if tag == \"NN\" or tag == \"VBZ\":\n",
        "        words.append((start, end, tag))\n",
        "\n",
        "print(words)\n"
      ],
      "metadata": {
        "id": "OGVW7RdTsXXN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "286d66ba-4c5e-4b11-caa9-e4e599af985d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 3, 'NN'), (4, 9, 'VBZ'), (17, 21, 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install textstat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nLgK6AY-AXn",
        "outputId": "84467ddb-0970-4527-f3c3-998c2477f0c7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textstat\n",
            "  Downloading textstat-0.7.3-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 kB\u001b[0m \u001b[31m626.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyphen (from textstat)\n",
            "  Downloading pyphen-0.14.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
            "Successfully installed pyphen-0.14.0 textstat-0.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textstat\n",
        "\n",
        "def calculate_readability_scores(text):\n",
        "    \"\"\"\n",
        "    Calculate various readability scores for the given text.\n",
        "    \"\"\"\n",
        "    scores = {\n",
        "        \"Flesch Reading Ease\": textstat.flesch_reading_ease(text),\n",
        "        \"Flesch-Kincaid Grade Level\": textstat.flesch_kincaid_grade(text),\n",
        "        \"Gunning Fog\": textstat.gunning_fog(text),\n",
        "        \"SMOG Index\": textstat.smog_index(text),\n",
        "        \"Automated Readability Index\": textstat.automated_readability_index(text),\n",
        "        \"Coleman-Liau Index\": textstat.coleman_liau_index(text),\n",
        "        \"Linsear Write Formula\": textstat.linsear_write_formula(text),\n",
        "        \"Dale-Chall Readability Score\": textstat.dale_chall_readability_score(text)\n",
        "    }\n",
        "    return scores\n",
        "\n",
        "\n",
        "# Checking Readability score for the rule, which convert a declarative sentence into a rhetorical question.\n",
        "print(\"\\nNow checking the readability of the opinion which is created by declarative sentence into a rhetorical question\\n\")\n",
        "for i in enhanced_example_sentences:\n",
        "    code_string = i\n",
        "    print(\"-\"*70)\n",
        "    print(\"Fact: \", i)\n",
        "    readability_scores = calculate_readability_scores(code_string)\n",
        "    for test, score in readability_scores.items():\n",
        "        print(f\"{test}: {score}\")\n",
        "\n",
        "    print(\"*\"*45)\n",
        "\n",
        "    opinion = convert_to_rhetorical_question_enhanced(i)\n",
        "    print(\"Opinion: \", opinion)\n",
        "    readability_scores = calculate_readability_scores(opinion)\n",
        "    for test, score in readability_scores.items():\n",
        "        print(f\"{test}: {score}\")\n",
        "    print(\"-\"*70)\n",
        "\n",
        "\n",
        "# Checking Readability score for the rule, which changes the structure of the sentence by adding phrases like \"In my opinion, In my view, \".\n",
        "print(\"\\nNow checking the readability of the opinion which is created by using adjectives and adverbs\\n\")\n",
        "for i in enhanced_example_sentences:\n",
        "    code_string = i\n",
        "    print(\"-\"*70)\n",
        "    print(\"Fact: \", i)\n",
        "    readability_scores = calculate_readability_scores(code_string)\n",
        "    for test, score in readability_scores.items():\n",
        "        print(f\"{test}: {score}\")\n",
        "\n",
        "    print(\"*\"*45)\n",
        "\n",
        "    opinion = make_sentence_opinionated(i)\n",
        "    print(\"Opinion: \", opinion)\n",
        "    readability_scores = calculate_readability_scores(opinion)\n",
        "    for test, score in readability_scores.items():\n",
        "        print(f\"{test}: {score}\")\n",
        "    print(\"-\"*70)\n",
        "\n",
        "# Print the scores\n",
        "\n"
      ],
      "metadata": {
        "id": "_sO-lco8RLZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2IBTkGC-Tt_",
        "outputId": "b84af250-bc5c-49fb-e681-ce43f138b61e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "import torch"
      ],
      "metadata": {
        "id": "HGxAt-Ji-hE8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWBRpQnW-nZK",
        "outputId": "b2a7976b-2f9d-4f2f-d452-588b737e3080"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Your sentence\n",
        "sentence = \"How you Mike?\"\n",
        "\n",
        "# Tokenize and create tensor\n",
        "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "labels = tokenizer(sentence, return_tensors=\"pt\")[\"input_ids\"]\n",
        "\n",
        "# Predict and get the most likely token indices\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs, labels=labels)\n",
        "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "\n",
        "# Decode and print the sentence\n",
        "corrected_sentence = tokenizer.decode(predictions[0])\n",
        "print(\"Corrected Sentence:\", corrected_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yndhzsKl_ATu",
        "outputId": "fefd6db7-c523-4214-b230-8405238af88c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corrected Sentence: . how you mike?.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install happytransformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWRXxxflAp0s",
        "outputId": "d72fda28-dc18-4ad9-f3cc-f113beafc72e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: happytransformer in /usr/local/lib/python3.10/dist-packages (3.0.0)\n",
            "Requirement already satisfied: torch>=1.0 in /usr/local/lib/python3.10/dist-packages (from happytransformer) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.43 in /usr/local/lib/python3.10/dist-packages (from happytransformer) (4.66.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.30.1 in /usr/local/lib/python3.10/dist-packages (from happytransformer) (4.35.2)\n",
            "Requirement already satisfied: datasets<3.0.0,>=2.13.1 in /usr/local/lib/python3.10/dist-packages (from happytransformer) (2.16.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from happytransformer) (0.1.99)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from happytransformer) (3.20.3)\n",
            "Requirement already satisfied: accelerate<1.0.0,>=0.20.1 in /usr/local/lib/python3.10/dist-packages (from happytransformer) (0.26.1)\n",
            "Requirement already satisfied: tokenizers<1.0.0,>=0.13.3 in /usr/local/lib/python3.10/dist-packages (from happytransformer) (0.15.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (from happytransformer) (0.16.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate<1.0.0,>=0.20.1->happytransformer) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate<1.0.0,>=0.20.1->happytransformer) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate<1.0.0,>=0.20.1->happytransformer) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate<1.0.0,>=0.20.1->happytransformer) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate<1.0.0,>=0.20.1->happytransformer) (0.20.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate<1.0.0,>=0.20.1->happytransformer) (0.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.13.1->happytransformer) (3.13.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.13.1->happytransformer) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.13.1->happytransformer) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.13.1->happytransformer) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.13.1->happytransformer) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.13.1->happytransformer) (2.31.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.13.1->happytransformer) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.13.1->happytransformer) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.13.1->happytransformer) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.13.1->happytransformer) (3.9.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->happytransformer) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->happytransformer) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->happytransformer) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->happytransformer) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->happytransformer) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.30.1->happytransformer) (2023.6.3)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb->happytransformer) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->happytransformer) (3.1.41)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->happytransformer) (1.39.2)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb->happytransformer) (0.4.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb->happytransformer) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb->happytransformer) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb->happytransformer) (1.4.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb->happytransformer) (1.16.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,>=2.13.1->happytransformer) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,>=2.13.1->happytransformer) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,>=2.13.1->happytransformer) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,>=2.13.1->happytransformer) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,>=2.13.1->happytransformer) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,>=2.13.1->happytransformer) (4.0.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb->happytransformer) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets<3.0.0,>=2.13.1->happytransformer) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets<3.0.0,>=2.13.1->happytransformer) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets<3.0.0,>=2.13.1->happytransformer) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets<3.0.0,>=2.13.1->happytransformer) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0->happytransformer) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets<3.0.0,>=2.13.1->happytransformer) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets<3.0.0,>=2.13.1->happytransformer) (2023.3.post1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0->happytransformer) (1.3.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->happytransformer) (5.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from happytransformer import HappyTextToText, TTSettings\n",
        "\n",
        "happy_tt = HappyTextToText(\"T5\", \"vennify/t5-base-grammar-correction\")\n",
        "\n",
        "args = TTSettings(num_beams=5, min_length=1)\n",
        "\n",
        "# Add the prefix \"grammar: \" before each input\n",
        "result = happy_tt.generate_text(\"grammar: Aren't I excited about the trip??\", args=args)\n",
        "\n",
        "print(result.text) # This sentence has bad grammar.\n"
      ],
      "metadata": {
        "id": "0wRSbBFoAwmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aLVHPEvFI91",
        "outputId": "943eaa59-61fe-4a10-b004-02db0e6822eb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "model_name = 'deep-learning-analytics/GrammarCorrector'\n",
        "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
        "\n",
        "def correct_grammar_model(input_text,num_return_sequences):\n",
        "  batch = tokenizer([input_text],truncation=True,padding='max_length',max_length=64, return_tensors=\"pt\").to(torch_device)\n",
        "  translated = model.generate(**batch,max_length=64,num_beams=num_beams, num_return_sequences=num_return_sequences, temperature=1.5)\n",
        "  tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
        "  return tgt_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuWF4JfYEY4K",
        "outputId": "2cc68ec8-4f46-454d-b455-35b5b891af5d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'He are moving here.'\n",
        "print(correct_grammar_model(text, num_return_sequences=2))\n",
        "['He is moving here.', 'He is moving here now.']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "Yg72UnBkGayz",
        "outputId": "5b0e8922-06bf-497f-dcf9-0d773bbee7e0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'num_beams' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-9c5ef2a1dbcc>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'He are moving here.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect_grammar_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_return_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'He is moving here.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'He is moving here now.'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-93481e235771>\u001b[0m in \u001b[0;36mcorrect_grammar_model\u001b[0;34m(input_text, num_return_sequences)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcorrect_grammar_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_return_sequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max_length'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mtranslated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_beams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_beams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_return_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_return_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0mtgt_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtgt_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'num_beams' is not defined"
          ]
        }
      ]
    }
  ]
}